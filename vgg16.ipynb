{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load the pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Freeze only the first few convolutional layers\nfor i, param in enumerate(vgg16.features[:10]):  # Freeze the first 10 layers\n    param.requires_grad = False\n\n# Modify the classifier part for binary classification\nvgg16.classifier[6] = nn.Linear(4096, 1)  # Replace the last layer with a binary classifier (1 output for binary)\nvgg16.classifier.add_module('sigmoid', nn.Sigmoid())  # Add a sigmoid activation for binary classification\n\n# Check which parameters are trainable\nfor name, param in vgg16.named_parameters():\n    if param.requires_grad:\n        print(f\"{name} is trainable\")\n\n# Example of moving the model to a GPU (if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg16 = vgg16.to(device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\n# Load pre-trained VGG16 model\nvgg16 = models.vgg16(pretrained=True)\n\n# Modify the input layer for 32x32x3 input\nvgg16.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n\n# Freeze the first few convolutional layers (e.g., first 10 layers)\nfor i, param in enumerate(vgg16.features[:10]):  # Freeze the first 10 layers\n    param.requires_grad = False\n\n# Modify the classifier part for the new input size after flattening\nvgg16.classifier = nn.Sequential(\n    nn.Linear(512 * 1 * 1, 4096),  # Adjust for 32x32 input\n    nn.ReLU(True),\n    nn.Dropout(),\n    nn.Linear(4096, 4096),\n    nn.ReLU(True),\n    nn.Dropout(),\n    nn.Linear(4096, 1),  # Binary classification output\n    nn.Sigmoid()  # Sigmoid for binary classification\n)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvgg16 = vgg16.to(device)\n\n# Print trainable layers (for verification)\nfor name, param in vgg16.named_parameters():\n    if param.requires_grad:\n        print(f\"{name} is trainable\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n# Binary classification, so no need for num_classes\ncriterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\ncriterion = criterion.to(device)  # Move to device (GPU if available)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define the hyperparameters\nlearning_rate = 0.001  # η\nbeta1 = 0.9  # β1\nbeta2 = 0.999  # β2\nepsilon = 1e-7  # ε\n\n# Create an instance of the Adam optimizer, but only pass the trainable parameters\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, vgg16.parameters()), \n                       lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\n\n# Load your pretrained model weights here if needed\nmodel.load_state_dict(torch.load('Adam_weights.pth'))\n\n# Create empty lists to store training and validation losses and accuracies\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\n# Set the current epoch number to continue from where you left off\ncurrent_epoch = 0  # Change this to the epoch you want to continue from\n\nnum_epochs = 2  # Set the total number of epochs you want to run\n\n# Define early stopping parameters\npatience = 5  # Number of epochs with no improvement after which training will stop\nmin_delta = 0.001  # Minimum change in validation loss to be considered as improvement\nbest_val_loss = float('inf')\nepochs_without_improvement = 0\n\n# Continue training for additional epochs\nfor epoch in range(current_epoch, num_epochs):\n    model.train()\n    running_train_loss = 0.0\n    train_preds = []\n    train_targets = []\n\n    # Training loop with a progress bar\n    for images, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Training)'):\n        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels.float())  # Convert labels to float for BCEWithLogitsLoss\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        running_train_loss += loss.item()\n\n        # Convert logits to probabilities with sigmoid\n        probs = torch.sigmoid(outputs)\n        predicted = (probs > 0.5).float()  # Convert probabilities to binary (0 or 1)\n        \n        train_preds.extend(predicted.cpu().numpy())\n        train_targets.extend(labels.cpu().numpy())\n\n    # Calculate and print the average training loss for this epoch\n    avg_train_loss = running_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    \n    # Calculate training accuracy for this epoch\n    train_accuracy = accuracy_score(train_targets, train_preds)\n    train_accuracies.append(train_accuracy)\n\n    model.eval()  # Set the model to evaluation mode\n    running_val_loss = 0.0\n    val_preds = []\n    val_targets = []\n\n    # Validation loop with a progress bar\n    for images, labels in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} (Validation)'):\n        images, labels = images.to(device), labels.to(device)  # Move data to GPU\n        with torch.no_grad():\n            outputs = model(images)\n            loss = criterion(outputs, labels.float())  # Use float labels for BCEWithLogitsLoss\n            running_val_loss += loss.item()\n\n            # Convert logits to probabilities with sigmoid\n            probs = torch.sigmoid(outputs)\n            predicted = (probs > 0.5).float()  # Convert probabilities to binary (0 or 1)\n\n            val_preds.extend(predicted.cpu().numpy())\n            val_targets.extend(labels.cpu().numpy())\n\n    # Calculate and print the average validation loss for this epoch\n    avg_val_loss = running_val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n    \n    # Calculate validation accuracy for this epoch\n    val_accuracy = accuracy_score(val_targets, val_preds)\n    val_accuracies.append(val_accuracy)\n\n    # Early stopping\n    if avg_val_loss + min_delta < best_val_loss:\n        best_val_loss = avg_val_loss\n        epochs_without_improvement = 0\n        torch.save(model.state_dict(), 'Adam_weights.pth')\n    else:\n        epochs_without_improvement += 1\n        if epochs_without_improvement >= patience:\n            print(f'Early stopping after {patience} epochs without improvement.')\n            break\n\n    # Print loss and accuracy for this epoch\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}')\n    print(f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n\nprint('Finished Training')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training vs Validation accuracies\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(range(1, num_epochs + 1), train_accuracies, label='Train_Accuracy', marker='o')\nplt.plot(range(1, num_epochs + 1), val_accuracies, label='Val_Accuracy', marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Training vs. Validation Accuracies')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training vs Validation loss\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\nplt.plot(range(1, num_epochs + 1), train_losses, label='Train_Loss')\nplt.plot(range(1, num_epochs + 1), val_losses, label='Val_Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training vs Validation Loss')\nplt.grid(True)\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, roc_curve, confusion_matrix, classification_report, f1_score, recall_score\n\n# Initialize empty lists to store true labels and predicted class probabilities\ntrue_labels = []\npredicted_probs = []\n\n# Set the model to evaluation mode\nmodel.eval()\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)  # Move images to GPU\n        labels = labels.to(device)  # Move labels to GPU (optional, depending on usage)\n        outputs = model(images)\n\n        # Get the predicted class probabilities using sigmoid\n        probs = torch.sigmoid(outputs).cpu().numpy().flatten()  # Convert logits to probabilities\n        predicted_probs.extend(probs)\n\n        # Store true labels\n        true_labels.extend(labels.cpu().numpy())\n\n# Convert probabilities to binary labels (0 or 1) using a threshold of 0.5\npredicted_labels = [1 if prob >= 0.5 else 0 for prob in predicted_probs]\n\n# Calculate the accuracy\naccuracy = accuracy_score(true_labels, predicted_labels)\n\n# Calculate the AUC\nroc_auc = roc_auc_score(true_labels, predicted_probs)\n\n# Calculate precision, recall, and F1 score\nprecision = precision_score(true_labels, predicted_labels)\nrecall = recall_score(true_labels, predicted_labels)\nf1 = f1_score(true_labels, predicted_labels)\n\n# Calculate sensitivity (True Positive Rate)\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\nsensitivity = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0])\n\n# Calculate specificity (True Negative Rate)\nspecificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n\n# Print the results\nprint(f'AUC: {roc_auc:.4f}')\nprint(f'Accuracy: {accuracy:.4f}')\nprint(f'Precision: {precision:.4f}')\nprint(f'Recall (Sensitivity): {recall:.4f}')\nprint(f'Specificity: {specificity:.4f}')\nprint(f'F1 Score: {f1:.4f}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# ROC Curve\nfpr, tpr, thresholds = roc_curve(true_labels, predicted_probs)\nplt.figure(figsize=(6, 6))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc='lower right')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Access values from the confusion matrix\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\nFN = conf_matrix[1, 0]\nTP = conf_matrix[1, 1]\n\n# Now you can use these values as needed\n\n# Define the confusion matrix\nconf_matrix = np.array([[TN, FP], [FN, TP]])  \n\n# Confusion Matrix\nplt.figure(figsize=(6, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n\n# Annotate cells with their values\nthresh = conf_matrix.max() / 2.0  # Set a threshold for text coloring\nfor i in range(conf_matrix.shape[0]):\n    for j in range(conf_matrix.shape[1]):\n        plt.text(j, i, str(conf_matrix[i, j]), horizontalalignment=\"center\",\n                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n\nplt.title('Confusion Matrix')\nplt.colorbar()\nplt.xticks([0, 1], ['Predicted 0', 'Predicted 1'])\nplt.yticks([0, 1], ['Actual 0', 'Actual 1'])\nplt.xlabel('True labels')\nplt.ylabel('Predicted labels')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]}]}